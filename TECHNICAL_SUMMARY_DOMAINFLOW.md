Here is the portfolio entry for DomainFlow.I have maintained the consistent style used for the previous project, highlighting the System Architecture, Multi-Model AI Integration, and Complex Algorithms (like the Cartesian product generation).ðŸ§  DomainFlowAI-Powered Domain Modeling & Requirements EngineQuick Summary: An enterprise-grade SaaS platform that automates the software design lifecycle. It orchestrates multiple Large Language Models (GPT-4o & Gemini 1.5) to transform high-level project descriptions into structured domain models, interactive UML diagrams, and comprehensive use case specifications.ðŸ› ï¸ Tech StackFrontend: React 18, Vite, Material-UI, Konva (Canvas/UML Visualization)Backend: Node.js, Express.js (REST API)Database: MySQL 8.0 (AWS RDS), Redis (Caching)AI Layer: OpenAI GPT-4o (Logic/Validation), Google Gemini 1.5 Pro (Context Generation)Infrastructure: AWS RDS, JWT Auth, DockerðŸ—ï¸ System ArchitectureThe application follows a Layered Monolithic Architecture designed for high data integrity and complex relationship management.Client Layer: A React SPA handling complex state for interactive matrices (DSM) and canvas rendering (UML).API Layer: Express.js middleware manages Authentication (RBAC) and routing.Intelligence Layer: A dedicated service module routes prompts to specific AI providers based on strength (e.g., GPT-4o for logic validation, Gemini for creative persona generation).Data Layer: A normalized MySQL schema (30+ tables) stores strict relationships between Entities, Features, and Actors.ðŸ›¡ï¸ Key Technical Highlights1. Multi-Model AI OrchestrationRather than relying on a single model, I engineered a hybrid approach to optimize for cost and accuracy:OpenAI GPT-4o: Used for deterministic tasks like generating Lean Canvas sections, defining strict MoSCoW feature sets, and validating business logic rules.Google Gemini 1.5 Pro: Utilized for high-context generation, specifically for fleshing out detailed User Personas, Use Case preconditions, and Triggers where large context windows are beneficial.2. Algorithmic Type Combination AnalysisA core feature is the ability to validate complex business rules. I implemented a Cartesian Product Algorithm to generate every possible combination of entity types.The system generates thousands of combinations (e.g., UserType Ã— SubscriptionStatus Ã— Region).These are batch-processed (100 items/batch) through GPT-4o to automatically flag "Impossible" or "Special Case" combinations, reducing manual QA time by 90%.3. Interactive Visualization EngineBuilt a custom drag-and-drop UML modeler using Konva.js.State synchronization ensures that when an Entity is updated in the database, the visual node on the canvas updates in real-time.Implemented a Design Structure Matrix (DSM) to visualize and manage complex $N \times N$ dependencies between system features.ðŸ’» Code Highlight: Cartesian Product GenerationA snippet showing the core logic for generating testable combinations before sending them to the AI validation queue.JavaScript/**
 * Generates Cartesian product of entity types for validation.
 * Used to create exhaustive test cases for AI analysis.
 */
function generateCombinations(entities) {
  // 1. Extract type arrays from selected entities
  const typeArrays = entities.map(e => e.types);
  
  // 2. Reduce into cartesian product
  // Input: [[A1, A2], [B1, B2]] -> Output: [[A1, B1], [A1, B2], [A2, B1], [A2, B2]]
  const combinations = typeArrays.reduce((acc, curr) => 
    acc.flatMap(d => curr.map(e => [...d, e])), 
    [[]]
  );

  // 3. Hash content for duplicate detection (SHA-256)
  return combinations.map(combo => ({
    hash: crypto.createHash('sha256').update(JSON.stringify(combo)).digest('hex'),
    data: combo
  }));
}
ðŸš§ Challenges & SolutionsChallenge: Managing AI Hallucinations & Data ConsistencyProblem: AI models often returned JSON with inconsistent keys or structure, breaking the frontend parsers.Solution: Implemented Zod schema validation on all AI responses. I also developed an "Upsert Logic" strategy: the system checks if an AI-generated entity already exists (fuzzy matching) and updates it rather than creating duplicates, preserving referential integrity in the SQL database.Challenge: Large Context Windows for Use CasesProblem: Generating accurate "Post-conditions" requires knowing the Actors, Pre-conditions, and Triggers. Sending full project history exceeded token limits.Solution: Implemented a Context Pruning Strategy. Before sending a prompt to Gemini, the backend recursively fetches only the directly related dependencies from the database, sanitizes circular references, and truncates non-essential descriptions to fit within the context window while maintaining relevance.